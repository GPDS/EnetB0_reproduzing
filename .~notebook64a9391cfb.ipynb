{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in /home/emanuel/.local/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /home/emanuel/.local/lib/python3.10/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /home/emanuel/.local/lib/python3.10/site-packages (from imbalanced-learn) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /home/emanuel/.local/lib/python3.10/site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /home/emanuel/.local/lib/python3.10/site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /home/emanuel/.local/lib/python3.10/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /home/emanuel/.local/lib/python3.10/site-packages (from imbalanced-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /home/emanuel/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/emanuel/.local/lib/python3.10/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/emanuel/.local/lib/python3.10/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/emanuel/.local/lib/python3.10/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/emanuel/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/emanuel/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/emanuel/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.43.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/emanuel/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/emanuel/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/emanuel/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/emanuel/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/emanuel/.local/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/emanuel/.local/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting lime\n",
      "  Using cached lime-0.2.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: matplotlib in /home/emanuel/.local/lib/python3.10/site-packages (from lime) (3.9.2)\n",
      "Requirement already satisfied: numpy in /home/emanuel/.local/lib/python3.10/site-packages (from lime) (1.26.4)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /home/emanuel/.local/lib/python3.10/site-packages (from lime) (0.22.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /home/emanuel/.local/lib/python3.10/site-packages (from lime) (1.5.1)\n",
      "Requirement already satisfied: scipy in /home/emanuel/.local/lib/python3.10/site-packages (from lime) (1.11.3)\n",
      "Collecting tqdm (from lime)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/emanuel/.local/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /home/emanuel/.local/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (10.0.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /home/emanuel/.local/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/emanuel/.local/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (2023.12.9)\n",
      "Requirement already satisfied: packaging>=21 in /home/emanuel/.local/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (23.1)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /home/emanuel/.local/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (0.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/emanuel/.local/lib/python3.10/site-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/emanuel/.local/lib/python3.10/site-packages (from scikit-learn>=0.18->lime) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/emanuel/.local/lib/python3.10/site-packages (from matplotlib->lime) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/emanuel/.local/lib/python3.10/site-packages (from matplotlib->lime) (0.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/emanuel/.local/lib/python3.10/site-packages (from matplotlib->lime) (4.43.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/emanuel/.local/lib/python3.10/site-packages (from matplotlib->lime) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->lime) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/emanuel/.local/lib/python3.10/site-packages (from matplotlib->lime) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, lime\n",
      "Successfully installed lime-0.2.0.1 tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n",
    "!pip install seaborn\n",
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import shutil\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from lime.lime_image import LimeImageExplainer\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from lime.lime_image import LimeImageExplainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_paths(dir):\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    folds = os.listdir(dir)\n",
    "    for fold in folds:\n",
    "        foldpath = os.path.join(dir, fold)\n",
    "        filelist = os.listdir(foldpath)\n",
    "        for file in filelist:\n",
    "            fpath = os.path.join(foldpath, file)\n",
    "            filepaths.append(fpath)\n",
    "            labels.append(fold)\n",
    "    return filepaths, labels\n",
    "\n",
    "def define_df(files, classes):\n",
    "    Fseries = pd.Series(files, name='filepaths')\n",
    "    Lseries = pd.Series(classes, name='labels')\n",
    "    return pd.concat([Fseries, Lseries], axis=1)\n",
    "\n",
    "def create_df(tr_dir, val_dir, ts_dir):\n",
    "    train_files, train_classes = file_paths(tr_dir)\n",
    "    train_df = define_df(train_files, train_classes)\n",
    "\n",
    "    val_files, val_classes = file_paths(val_dir)\n",
    "    valid_df = define_df(val_files, val_classes)\n",
    "\n",
    "    test_files, test_classes = file_paths(ts_dir)\n",
    "    test_df = define_df(test_files, test_classes)\n",
    "    \n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_generators(train_df, valid_df, test_df, batch_size):\n",
    "    image_size = (224, 224)\n",
    "    num_channels = 3\n",
    "    image_shape = (image_size[0], image_size[1], num_channels)\n",
    "    \n",
    "    test_len = len(test_df)\n",
    "    optimal_test_batch_size = max([test_len // n for n in range(1, test_len + 1) if test_len % n == 0 and test_len / n <= 80])\n",
    "    test_steps = test_len // optimal_test_batch_size\n",
    "\n",
    "    def preprocess_image(img):\n",
    "        return img\n",
    "\n",
    "   \n",
    "    train_datagen = ImageDataGenerator(preprocessing_function=preprocess_image, horizontal_flip=True)\n",
    "    valid_test_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='filepaths',\n",
    "        y_col='labels',\n",
    "        target_size=image_size,\n",
    "        class_mode='categorical',\n",
    "        color_mode='rgb',\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    valid_generator = valid_test_datagen.flow_from_dataframe(\n",
    "        dataframe=valid_df,\n",
    "        x_col='filepaths',\n",
    "        y_col='labels',\n",
    "        target_size=image_size,\n",
    "        class_mode='categorical',\n",
    "        color_mode='rgb',\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_generator = valid_test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        x_col='filepaths',\n",
    "        y_col='labels',\n",
    "        target_size=image_size,\n",
    "        class_mode='categorical',\n",
    "        color_mode='rgb',\n",
    "        shuffle=False,\n",
    "        batch_size=optimal_test_batch_size\n",
    "    )\n",
    "\n",
    "    return train_generator, valid_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(generator):\n",
    "    \n",
    "    class_dict = generator.class_indices\n",
    "    class_names = list(class_dict.keys())\n",
    "\n",
    "    \n",
    "    images, labels = next(generator)\n",
    "\n",
    "  \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    num_images = len(labels)\n",
    "    num_samples = min(num_images, 25)\n",
    "\n",
    "   \n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        img = images[i] / 255.0  \n",
    "        plt.imshow(img)\n",
    "        \n",
    "        \n",
    "        class_index = np.argmax(labels[i])\n",
    "        class_label = class_names[class_index]\n",
    "\n",
    "        \n",
    "        plt.title(class_label, color='blue', fontsize=12)\n",
    "        plt.axis('off')\n",
    "\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model, base_model, patience, stop_patience, threshold, factor, batches, initial_epoch, epochs):\n",
    "        super(MyCallback, self).__init__()\n",
    "        self.model = model\n",
    "        self.base_model = base_model\n",
    "        self.patience = patience \n",
    "        self.stop_patience = stop_patience \n",
    "        self.threshold = threshold \n",
    "        self.factor = factor \n",
    "        self.batches = batches \n",
    "        self.initial_epoch = initial_epoch\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.count = 0 \n",
    "        self.stop_count = 0\n",
    "        self.best_epoch = 1  \n",
    "        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.lr)) \n",
    "        self.highest_tracc = 0.0 \n",
    "        self.lowest_vloss = np.inf \n",
    "        self.best_weights = self.model.get_weights() \n",
    "        self.initial_weights = self.model.get_weights()   \n",
    "\n",
    "   \n",
    "    def on_train_begin(self, logs= None):\n",
    "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
    "        print(msg)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_end(self, logs= None):\n",
    "        stop_time = time.time()\n",
    "        tr_duration = stop_time - self.start_time\n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print(msg)\n",
    "        self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs= None):\n",
    "        acc = logs.get('accuracy') * 100\n",
    "        loss = logs.get('loss')\n",
    "        msg = '{0:20s}processing batch {1:} of {2:5s}-   accuracy=  {3:5.3f}   -   loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
    "        print(msg, '\\r', end= '') \n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs= None):\n",
    "        self.ep_start = time.time()\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs= None):\n",
    "        ep_end = time.time()\n",
    "        duration = ep_end - self.ep_start\n",
    "\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr)) \n",
    "        current_lr = lr\n",
    "        acc = logs.get('accuracy')  \n",
    "        v_acc = logs.get('val_accuracy')  \n",
    "        loss = logs.get('loss')  \n",
    "        v_loss = logs.get('val_loss')  \n",
    "\n",
    "        if acc < self.threshold: \n",
    "            monitor = 'accuracy'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "            else:\n",
    "                pimprov = (acc - self.highest_tracc ) * 100 / self.highest_tracc \n",
    "\n",
    "            if acc > self.highest_tracc: \n",
    "                self.highest_tracc = acc \n",
    "                self.best_weights = self.model.get_weights()\n",
    "                self.count = 0 \n",
    "                self.stop_count = 0 \n",
    "                if v_loss < self.lowest_vloss:\n",
    "                    self.lowest_vloss = v_loss\n",
    "                self.best_epoch = epoch + 1  \n",
    "\n",
    "            else:\n",
    "              \n",
    "                if self.count >= self.patience - 1: \n",
    "                    lr = lr * self.factor \n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) \n",
    "                    self.count = 0 \n",
    "                    self.stop_count = self.stop_count + 1 \n",
    "                    self.count = 0 \n",
    "                    if v_loss < self.lowest_vloss:\n",
    "                        self.lowest_vloss = v_loss\n",
    "                else:\n",
    "                    self.count = self.count + 1 \n",
    "        else: \n",
    "            monitor = 'val_loss'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "            else:\n",
    "                pimprov = (self.lowest_vloss - v_loss ) * 100 / self.lowest_vloss\n",
    "            if v_loss < self.lowest_vloss: \n",
    "                self.lowest_vloss = v_loss #\n",
    "                self.best_weights = self.model.get_weights() \n",
    "                self.count = 0 \n",
    "                self.stop_count = 0\n",
    "                self.best_epoch = epoch + 1 \n",
    "            else: \n",
    "                if self.count >= self.patience - 1: \n",
    "                    lr = lr * self.factor \n",
    "                    self.stop_count = self.stop_count + 1 \n",
    "                    self.count = 0 #\n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) \n",
    "                else:\n",
    "                    self.count = self.count + 1\n",
    "                if acc > self.highest_tracc:\n",
    "                    self.highest_tracc = acc\n",
    "\n",
    "        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc * 100:^9.3f}{v_loss:^9.5f}{v_acc * 100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
    "        print(msg)\n",
    "\n",
    "        if self.stop_count > self.stop_patience - 1: \n",
    "            msg = f' training stopped at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
    "            print(msg)\n",
    "            self.model.stop_training = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(history):\n",
    "    training_accuracy = history.history['accuracy']\n",
    "    training_loss = history.history['loss']\n",
    "    validation_accuracy = history.history['val_accuracy']\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    \n",
    "    best_epoch_loss = np.argmin(validation_loss) + 1\n",
    "    best_epoch_accuracy = np.argmax(validation_accuracy) + 1\n",
    "    lowest_val_loss = validation_loss[best_epoch_loss - 1]\n",
    "    highest_val_acc = validation_accuracy[best_epoch_accuracy - 1]\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    epochs = range(1, len(training_accuracy) + 1)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, training_loss, 'r', label='Training Loss')\n",
    "    plt.plot(epochs, validation_loss, 'g', label='Validation Loss')\n",
    "    plt.scatter(best_epoch_loss, lowest_val_loss, s=150, c='blue', label=f'Best Epoch = {best_epoch_loss}')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, training_accuracy, 'r', label='Training Accuracy')\n",
    "    plt.plot(epochs, validation_accuracy, 'g', label='Validation Accuracy')\n",
    "    plt.scatter(best_epoch_accuracy, highest_val_acc, s=150, c='blue', label=f'Best Epoch = {best_epoch_accuracy}')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(cm, class_names, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print('Normalized Confusion Matrix')\n",
    "    else:\n",
    "        print('Confusion Matrix, Without Normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "   \n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], '.2f' if normalize else 'd'),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > threshold else 'black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = 'Train'\n",
    "validation_directory = 'Validation'\n",
    "test_directory = 'Test'\n",
    "\n",
    "train_dataframe, validation_dataframe, test_dataframe = create_df(train_directory, validation_directory, test_directory)\n",
    "\n",
    "batch_size = 40\n",
    "\n",
    "train_generator, validation_generator, test_generator = create_data_generators(train_dataframe, validation_dataframe, test_dataframe, batch_size)\n",
    "display_images(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(directory):\n",
    "    classes = []\n",
    "    counts = []\n",
    "    for class_folder in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_folder)\n",
    "        if os.path.isdir(class_path):\n",
    "            classes.append(class_folder)\n",
    "            counts.append(len(os.listdir(class_path)))\n",
    "    return classes, counts\n",
    "\n",
    "\n",
    "train_classes, train_counts = count_classes(train_directory)\n",
    "val_classes, val_counts = count_classes(validation_directory)\n",
    "test_classes, test_counts = count_classes(test_directory)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(train_classes, train_counts, color='b', alpha=0.5, label='Train')\n",
    "plt.bar(val_classes, val_counts, color='g', alpha=0.5, label='Validation')\n",
    "plt.bar(test_classes, test_counts, color='r', alpha=0.5, label='Test')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('No. of Samples')\n",
    "plt.title('Class distribution')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f\"Train class counts: {dict(zip(train_classes, train_counts))}\")\n",
    "print(f\"Validation class counts: {dict(zip(val_classes, val_counts))}\")\n",
    "print(f\"Test class counts: {dict(zip(test_classes, test_counts))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "valid_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_gen = valid_test_datagen.flow_from_directory(\n",
    "    validation_directory,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_gen = valid_test_datagen.flow_from_directory(\n",
    "    test_directory,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  \n",
    ")\n",
    "\n",
    "counter = Counter(train_gen.classes)\n",
    "print(\"Original class distribution in training set:\")\n",
    "print(counter)\n",
    "\n",
    "train_file_paths = np.array(train_gen.filepaths)\n",
    "train_labels = train_gen.classes.reshape(-1, 1)  \n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "train_file_paths_resampled, train_labels_resampled = oversampler.fit_resample(train_file_paths.reshape(-1, 1), train_labels)\n",
    "\n",
    "counter_resampled = Counter(train_labels_resampled.flatten())\n",
    "print(\"after oversampling:\")\n",
    "print(counter_resampled)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(counter_resampled.keys(), counter_resampled.values(), color='b', alpha=0.5)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('No. of Samples')\n",
    "plt.title('Class Distribution After Oversampling')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "train_file_paths_resampled, train_labels_resampled = oversampler.fit_resample(train_file_paths.reshape(-1, 1), train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_paths_resampled, train_labels_resampled = shuffle(train_file_paths_resampled, train_labels_resampled, random_state=42)\n",
    "\n",
    "train_labels_resampled = train_labels_resampled.astype(str)\n",
    "\n",
    "# Data Generators\n",
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "num_classes = 2  # Update this based on your dataset\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "valid_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_gen = valid_test_datagen.flow_from_directory(\n",
    "    validation_directory,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_gen = valid_test_datagen.flow_from_directory(\n",
    "    test_directory,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "def create_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(32, (3, 3), padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    'Train',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical')  \n",
    "\n",
    "valid_gen = valid_datagen.flow_from_directory(\n",
    "    'Validation',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical')  \n",
    "\n",
    "\n",
    "base_model = tf.keras.applications.EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(len(train_gen.class_indices), activation='softmax')(x)  \n",
    "\n",
    "\n",
    "# Compile and train the model\n",
    "input_shape = (image_size[0], image_size[1], 3)\n",
    "model = create_model(input_shape, num_classes)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=train_gen.samples // batch_size,\n",
    "    epochs=10,  # Adjust the number of epochs as needed\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps=valid_gen.samples // batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Dense(2, activation='softmax')(x)  \n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    'Train',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=train_gen.samples // batch_size,\n",
    "    epochs=10,  \n",
    "    validation_data=valid_gen,\n",
    "    validation_steps=valid_gen.samples // batch_size\n",
    ")\n",
    "\n",
    "loss, accuracy = model.evaluate(test_gen)\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history):\n",
    "   \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_training(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_length = len(test_gen)\n",
    "test_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "test_steps = ts_length // test_batch_size\n",
    "train_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
    "valid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
    "test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print(\"Validation Loss: \", valid_score[0])\n",
    "print(\"Validation Accuracy: \", valid_score[1])\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Confusion Matrix\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "# Generate predictions\n",
    "y_pred = model.predict(test_gen)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Compute confusion matrix and classification report\n",
    "target_names = ['hemorrhagic', 'ischaemic']\n",
    "cm = confusion_matrix(test_gen.classes, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_confusion_matrix(cm, classes=target_names, title='Confusion Matrix')\n",
    "\n",
    "print(classification_report(test_gen.classes, y_pred_classes, target_names=target_names))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training metrics\n",
    "def plot_training_metrics(history):\n",
    "    training_accuracy = history.history['accuracy']\n",
    "    training_loss = history.history['loss']\n",
    "    validation_accuracy = history.history['val_accuracy']\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    best_epoch_loss = np.argmin(validation_loss) + 1\n",
    "    best_epoch_accuracy = np.argmax(validation_accuracy) + 1\n",
    "    lowest_val_loss = validation_loss[best_epoch_loss - 1]\n",
    "    highest_val_acc = validation_accuracy[best_epoch_accuracy - 1]\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    epochs = range(1, len(training_accuracy) + 1)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, training_loss, 'r', label='Training Loss')\n",
    "    plt.plot(epochs, validation_loss, 'g', label='Validation Loss')\n",
    "    plt.scatter(best_epoch_loss, lowest_val_loss, s=150, c='blue', label=f'Best Epoch = {best_epoch_loss}')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, training_accuracy, 'r', label='Training Accuracy')\n",
    "    plt.plot(epochs, validation_accuracy, 'g', label='Validation Accuracy')\n",
    "    plt.scatter(best_epoch_accuracy, highest_val_acc, s=150, c='blue', label=f'Best Epoch = {best_epoch_accuracy}')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'StrokePredModel'\n",
    "subject = 'Stroke Prediction'\n",
    "save_path = ''\n",
    "acc = test_score[1] * 100\n",
    "\n",
    "save_id = str(f'{model_name}-{subject}-{\"%.2f\" %round(acc, 2)}.h5')\n",
    "model_save_loc = os.path.join(save_path, save_id)\n",
    "model.save(model_save_loc)\n",
    "print(f'model was saved successfully as {model_save_loc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeImageExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "\n",
    "    img = keras_image.array_to_img(image)\n",
    "    img = img.resize((224, 224))    \n",
    "    img_array = keras_image.img_to_array(img) / 255.0\n",
    "    return img_array\n",
    "\n",
    "explainer = LimeImageExplainer()\n",
    "\n",
    "num_samples = 5  \n",
    "for _ in range(num_samples):\n",
    " \n",
    "    images, _ = next(test_gen)  \n",
    "    image = images[0]   \n",
    "\n",
    "    preprocessed_image = preprocess_image(image)\n",
    " \n",
    "    preds = model.predict(np.array([preprocessed_image]))\n",
    "    pred_class = np.argmax(preds)\n",
    "  \n",
    "    explanation = explainer.explain_instance(preprocessed_image, model.predict, top_labels=1, hide_color=0, num_samples=1000)\n",
    "\n",
    "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
    "    plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class_dict = train_gen.class_indices\n",
    "height = []\n",
    "width = []\n",
    "for _ in range(len(class_dict)):\n",
    "    height.append(image_size[0])\n",
    "    width.append(image_size[1])\n",
    "\n",
    "Index_series = pd.Series(list(class_dict.values()), name='class_index')\n",
    "Class_series = pd.Series(list(class_dict.keys()), name='class')\n",
    "Height_series = pd.Series(height, name='height')\n",
    "Width_series = pd.Series(width, name='width')\n",
    "class_df = pd.concat([Index_series, Class_series, Height_series, Width_series], axis=1)\n",
    "\n",
    "# Define save_path; update it as per your requirement\n",
    "save_path = '/kaggle/working'  # Kaggle working directory\n",
    "\n",
    "subject = 'Stroke Prediction'\n",
    "csv_name = f'{subject}-Gov_dissertation.csv'\n",
    "csv_save_loc = os.path.join(save_path, csv_name)\n",
    "class_df.to_csv(csv_save_loc, index=False)\n",
    "print(f'class csv file was saved as {csv_save_loc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3900621,
     "sourceId": 6777633,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
